IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
3415
Resource Allocation With Edge Computing in IoT
Networks via Machine Learning
Xiaolan Liu , Student Member, IEEE, Jiadong Yu , Student Member, IEEE, Jian Wang, Member, IEEE, and Yue Gao, Senior Member, IEEE
Abstract—In this article, we investigate resource allocation with as well as smart farming by using a lot of IoT devices, like
edge computing in Internet-of-Things (IoT) networks via machine
sensors, actuators, and gateways to collect and process a large
learning approaches. Edge computing is playing a promising role
amount of data generated by the IoT networks [1]. Innovations in IoT networks by providing computing capabilities close to users.

in hardware and software in recent years have contributed
However, the massive number of users in IoT networks requires
sufficient spectrum resource to transmit their computation tasks
to the expansion of IoT networks with a large number of
to an edge server, while the IoT users were developed to have more
connected devices, which increases the requirements for data
powerful computation ability recently, which makes it possible for
processing, storage, and communications [2]. For example, them to execute some tasks locally. Then, the design of computation
in smart farming, many sensors are deployed to monitor the
task offloading policies for such IoT edge computing systems
environmental factors and animal welfare, like temperature,
remains challenging. In this article, centralized user clustering is
explored to group the IoT users into different clusters according to
humidity, level of lighting, noise, and gas levels. Generally,
users’ priorities. The cluster with the highest priority is assigned
the generated data by sensors are collected by the gateway and
to offload computation tasks and executed at the edge server, while
then sent to the cloud server to perform further processing [3].

the lowest priority cluster executes computation tasks locally. For
However, few challenges need to be addressed in this situ-
the other clusters, the design of distributed task offloading policies
ation. First, many farms are located in remote areas where
for the IoT users is modeled by a Markov decision process, where
each IoT user is considered as an agent which makes a series of
sensors might not be able to connect to the cloud services, edge
decisions on task offloading by minimizing the system cost based
computing, defined as providing computing capacities close
on the environment dynamics. To deal with the curse of high
to users, is a promising solution to complete the computation
dimensionality, we use a deep Q-network to learn the optimal chain [3], [4]. Second, a large number of sensors requests for policy in which deep neural network is used to approximate
computation resource from the edge server, which burdens on
the Q-function in Q-learning. Simulations show that users are grouped into clusters with optimal number of clusters. Moreover,
the spectrum resource and the computation capacity. Moreover,
our proposed computation offloading algorithm outperforms the
the IoT devices have been developed to have more powerful
other baseline schemes under the same system costs.

computation ability, which makes it possible for them to per-
Index Terms—Deep reinforcement learning (DRL), edge
form some simple data processing. Therefore, the design of the
computing, resource allocation, user clustering.

optimal computation task offloading scheme is necessary and
urgent in the IoT edge computing networks. A multiobjective
I. I
optimization problem has been formulated to jointly minimize
NTRODUCTION
the cost, including energy consumption, execution delay, and
A. Basic Knowledge
payment cost by finding the optimal offloading probability and
THE Internet of Things (IoT) has brought about a new era transmit power for each mobile device [5]. Moreover, game of smart applications, such as smart city, smart industry,
theory has been proposed to design a dynamic computation
offloading scheme in the fog computing system with energy
Manuscript received September 27, 2019; revised November 24, 2019
harvesting [6].

and January 7, 2020; accepted January 19, 2020. Date of publication
January 28, 2020; date of current version April 14, 2020. This work was
Compared to the cloud server, edge computing was
supported in part by the Research and Innovation Bridges Cooperation
proposed to support latency-critical services and remote IoT
Program between the U.K. and China, in part by the Ministry of Science and applications. As mentioned above, even the IoT users have
Technology in China under Grant 2016YFE0124200, in part by the Innovate
U.K., in part by the Biotechnology and Biological Sciences Research Council more powerful computation capacities, they are still suf-under Grant BB/R005257/1, and in part by the Engineering and Physical
fering serious resource scarcity problems, such as limited
Sciences Research Council under Grant EP/R00711X/1 in the U.K. This arti-
battery power and CPU computation resource [7]. Thus, cle was presented in part at the IEEE International Communication Conference (ICC 2019). (Corresponding author: Yue Gao.)

offloading part of computation tasks to relatively resource-
Xiaolan Liu and Jiadong Yu are with the Department of Electronic
rich edge devices can meet the Quality-of-Service (QoS)
Engineering
and
Computer
Science,
Queen
Mary
University
of
requirements of different applications as well as augment
London,
London
E1
4NS,
U.K.

(e-mail:
xiaolan.liu@qmul.ac.uk;
jiadong.yu@qmul.ac.uk).

the capabilities of IoT users for running resource-demanding
Jian Wang is with the School of Electronic Science and Engineering,
applications [8]. However, the edge device, i.e., the gate-Nanjing University, Nanjing 210023, China (e-mail: wangjnju@nju.edu.cn).

way in IoT networks, cannot support a large number of IoT
Yue Gao is with the Institute for Communication Systems, University of
Surrey, Guildford GU2 7XH, U.K. (email: yue.gao@ieee.org).

users to offload their computation tasks together due to the
Digital Object Identifier 10.1109/JIOT.2020.2970110
finite spectrum resource [9]. Hence, grouping the IoT users 2327-4662 c
2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

See https://www.ieee.org/publications/rights/index.html for more information.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.

3416
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
into different clusters could be an effective solution, which
derived, and the optimal policy was proved to have a threshold-
deals with the IoT users separately according to their differ-
based structure with respect to this defined function. Therefore,
ent demands on computation offloading in different clusters.

the users were classified into different groups with different
This can be achieved by using the clustering algorithms which
priorities above or below a given threshold perform complete
classify data points into different clusters, and the data points
and minimum offloading, respectively. Clustering algorithms,
in the same cluster have similar characteristics while they are
such as K-means [20], known to cluster data set into different different from data points in the other clusters [10].

clusters according to the characteristics of each data point,
Therefore, designing an optimal computation task offload-
make the way for user clustering in the IoT networks.

ing scheme, i.e., the task is executed at a local IoT user or
Machine learning approaches, such as unsupervised learn-
edge server, is the key challenge in the IoT edge comput-
ing, clustering algorithms, can extract the data structures
ing networks. It has been mostly discussed in the context
from a large amount of data; and reinforcement learning,
of mobile-edge computing (MEC), in which the mobile user
like Q-learning algorithm, can learn the optimal strategy by
makes a binary decision to either offload the computation
interacting with the environment. The increase of computa-
tasks to the edge device or not [11]. The conventional tion capacity at edge devices contributes to a new research
optimization methods, such as Lyapunov optimization and
area, called edge learning, which crosses and revolutionizes
convex optimization techniques, were adopted to solve the
two disciplines: 1) wireless communication and 2) machine
cost (e.g., energy consumption and task execution latency)
learning [21], [22]. Edge learning can be accomplished by minimization problem when exploring an optimal computation
leveraging the MEC platform. Some research works have been
offloading scheme [12]. However, they cannot make optimal studied to apply machine learning techniques to optimize the
decisions based on the dynamic environment. Note that design-
communication and computation task offloading schemes in
ing the computation task offloading scheme while interacting
the MEC network [14], [23]. In [24], an In-Edge AI has with the environment can be modeled as a Markov decision
been evaluated and could achieve near-optimal performance
process (MDP). Reinforcement learning is an effective method
by investigating the scenarios of edge caching and computa-
to solve an MDP problem without requiring the priori knowl-
tion offloading in MEC systems. An optimal task offloading
edge of environment statistics [13]. Moreover, to break the scheme was learned through a deep Q-network (DQN) by
curse of dimensionality, deep reinforcement learning (DRL)
maximizing the long-term utility performance without know-
is explored in the MEC system [14].

ing a priori knowledge of network dynamics [25]. In [26],
an intelligent resource allocation framework was developed
B. Related Work
to solve the complex resource allocation problem by propos-
In IoT networks, the emerging of edge computing has
ing a Monte Carlo tree search (MCTS) method based on a
brought many research challenges, such as dynamic compu-
multitask RL algorithm. An optimal computation offloading
tation offloading scheme design, resource (e.g., computing
scheme was proposed for an IoT device with energy harvest-
resource, spectrum resource) allocation, and transmit power
ing techniques to choose the MEC device and determine the
control. Moreover, they cannot be solved independently, such
computation offloading rate, with efficient resources, including
as designing an optimal computation offloading scheme has to
spectrum and power resource allocation in [27].

consider the limited resource of the gateway and the transmit
However, computation task offloading schemes in ultra-
power of the users. Computation task offloading scheme has
dense IoT networks with edge computing has rarely been
been investigated to access to multiresources efficiently in edge
considered. Designing computation task offloading schemes
computing networks, especially in the MEC system [15]–[17].

for IoT networks, especially for the applications with massive
Energy consumption and task execution latency are two main
IoT users, like smart farming, is necessary. Additionally, the
considerations when designing the optimal task offloading
optimal task offloading policy has been validated to have a
scheme. A joint optimization of task offloading scheduling
threshold-based structure [19], we consider using clustering and transmit power allocation scheme has been proposed in
algorithms to group the IoT users into different user clusters
the MEC system with single mobile user [15], while joint as the initial step of our proposed task offloading scheme.

optimizing radio and computational resource was considered
We define the clustering feature by user priority which is
with multiuser MEC system [16]. Liang et al. [18] optimized defined as a tuple, including the distance from user to the
the number of offloading users by exploring the computation
edge server and the task offloading probability. Moreover, the
offloading scheme for multiple users in the MEC system. The
distributed computation task offloading scheme is designed
conventional optimization tools were used to solve the task
with minimizing the system cost, task execution latency, and
offloading problem in [12], however, which cannot deal with energy consumption. In this article, we consider the weighted
the environment dynamics.

sum of task execution latency and energy consumption, which
Moreover, a lot of IoT users contending for the finite spec-
was rarely considered together before as the system cost.

trum and computation resource of the gateway makes it even
The design of the optimal computation offloading scheme
more hard to design the computation offloading schemes. The
is modeled as an MDP, where the objective is to minimize
problem of joint radio-and-computation resource allocation
the long-term system cost while considering the environment
in multiuser MEC systems in the presence of input/output
dynamics. We use a deep neural network (DNN) to approx-
interference was studied in [18]. In [19], an offloading priority imate the Q-function in Q-learning, and then a DQN-based
function which defined the priorities for users according to their
task offloading algorithm is proposed to learn the optimal task
channel gains and local computing energy consumption was
offloading policy.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



LIU et al. : RESOURCE ALLOCATION WITH EDGE COMPUTING IN IoT NETWORKS VIA MACHINE LEARNING
3417
The major contributions of this article are stated as follows.

1) We first design a computation task offloading scheme for
the ultradense IoT edge computing network via machine
learning approaches, which is achieved by two steps:
a) user clustering and b) distributed task offloading
algorithms.

2) We first cluster the IoT users into different clusters with
a K-means-based clustering algorithm according to the
clustering feature and user priority. The clusters with the
highest and lowest user priority are assigned as edge
computing and local computing, respectively.

3) For IoT users in the other clusters, the problem of
designing an optimal computation offloading policy is
modeled as an MDP, where the objective is to mini-
mize the long-term system cost while interacting with
the dynamic environment. Moreover, a DQN-based com-
putation offloading algorithm is proposed to learn the
optimal computation offloading policy.

4) The numerical results show that the IoT users are
grouped into different user clusters and the optimal
cluster number is validated. Also, the DQN-based com-
putation offloading algorithm is demonstrated to be
superior to the other baseline schemes.

The remainder of this article is organized as follows. In
Section II, the system model of computation task offload-
Fig. 1.

System model of IoT networks with edge computing.

ing in IoT edge computing networks is built. The problem
of task offloading with minimizing the system cost is formu-
lated in Section III. In Section IV, the centralized clustering
algorithm is applied to group the IoT users into different user
clusters. Then, a DQN-based optimal task offloading scheme
is proposed in Section V. The numerical results are given in
Section VI. Finally, the conclusions are drawn in Section VII.

II. SYSTEM MODEL
As shown in Fig. 1, an IoT edge computing network is Fig. 2.

Illustration of the computation offloading scheme with clustering
presented with a three-layer hierarchical architecture which
and RL.

consists of a cloud platform, multiple gateways, and a lot of
IoT users, which includes many independent IoT networks.

Each IoT network provides services for a large number of IoT
knowledge of the environment, like channel gains and other
users (i.e., different IoT devices), which is achieved by the
users’ decisions. First, in this article, the centralized cluster-
gateway (i.e., the edge devices) collecting data from IoT users
ing algorithm is proposed to group the IoT users into different
in its coverage area and processing them with its equipped
clusters according to user priorities, after that, the distributed
edge server. However, the edge server has finite computation
computation offloading scheme with reinforcement learning is
capacity and spectrum resource such that the gateway can only
proposed for IoT users in the clusters that need more deci-
allow a limited number of devices to be accessed at the same
sions. Hence, this computation offloading scheme has a time
time. Each IoT user generates computation tasks in different
phase-based structure as presented in Fig. 2, and we discrete sizes continuously and it has limited computation capacity and
time horizon into time slots in each time phase t and each
battery power as well, so offloading part of their computation
time slot is indexed by k.

tasks to the gateway may improve the computation experience
In the considered IoT networks, the IoT users transmit
in terms of energy consumption and task execution latency.

their data to the gateway by accessing to a limited number
As an example, a typical independent IoT network is con-
of channels with the bandwidth of each channel denoted by
sidered in this article, and the location distribution of IoT users
Bw. Assuming each IoT user continuously generates indepen-
is modeled by a Poisson cluster process (PCP). From Fig. 1,
dent computation tasks in different sizes. Each IoT user can
computation tasks can be either executed locally at the IoT
store a limited number of tasks with the maximum number
user or offloaded to the gateway and executed at the edge
of tasks that cannot exceed T, hence the possible number of
server. However, it is hard for the IoT users to make decisions
tasks stored at each IoT user in time slot k is denoted by
on computation offloading since it cannot know the perfect
T k = {1 , . . . , T}. The task generation is assumed to be an Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



3418
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
TABLE I
PARAMETERS NOTATION
independent and identically distributed sequence of Bernoulli
successfully, but there is a possibility that the task execution
random variables with a common parameter τ k ∈ [0 , 1]. The is failed, such as the task transmission might suffer from com-indicator of task generation is presented as Ik = {0 , 1}, munication outage between the IoT user and the gateway, so
where Ik = 1 indicates one task is generated with its task
we define a penalty function to illustrate this situation.

size following uniform distribution, denoted by a discrete set
Mk = { m 1 , . . . , mM}, otherwise, Ik = 0 means there is no A. Local Computing Mode task generated at current time slot. Here, τ k = Pr{ I = 1} =
We have Ok = 0 if the computation task is executed locally
1 − Pr{ I = 1}. Table I lists all the notations and descriptions at the IoT user. We let fd denote the fixed CPU frequency
of our considered system model.

of the IoT user, which presents the number of CPU cycles
required to compute one bit of input data. The energy con-
III. PROBLEM FORMULATION
sumption per CPU cycle is denoted by Ed. Then, fdEd indicates The benefits of distributing computing are releasing the
the computing energy consumption per bit at the IoT user.

computation burden on the cloud server, shrinking the task
The total energy consumption of one computation task at
execution latency and reducing the waste of spectrum resource.

the IoT user, denoted by Ek , is given by Ek
= f
.

cd
cd
d Ed mkj
Edge computing is going to make a great achievement of
Moreover, let Dd denote the computation capacity of the
these benefits when applied to the IoT networks. Moreover,
IoT user, which is measured by the number of CPU cycles
the more and more powerful computation chips are deployed
per second. The remaining CPU resource of the IoT user is
in IoT users, which makes it possible for local task execution.

denoted by the remaining percentage of computation resource
Therefore, the problem of solving the optimal computation
Rk = { r 1 , r 2 , . . . , 1}. The local computing latency Ld at the offloading policy, that is, the task is executed locally or
IoT user is defined as Lk = (f
)/D
d
d mkj
d . The energy consump-
offloaded to the edge server, remains challenging. In this
tion and the task execution latency are two main costs in the
section, we first introduce two task computing modes and
edge computing network. Then, we define the cost function of
then calculate the system cost under each computing mode,
the local computing mode for the IoT user as
respectively. Therefore, a typical IoT user is considered in
Ck = Ek + βLk
(1)
d
cd
d
an ultradense IoT network to learn the optimal computation
offloading policy by minimizing the long-term system cost.

where β indicates the weight factor between the energy con-
The design of the optimal computation task offloading
sumption and the task execution latency, which combines
scheme is to minimize the long-term system cost by choosing
different types of functions with different units into a universal
the best computing mode for each task generated by the IoT
cost function.

user, local computing mode or offloading computing mode.

The possible decisions of each IoT user on computation task
B. Edge Computing Mode
offloading in time slot k are Ok = {1} ∪ {0}, which indi-
Let gk denote channel gain from the IoT user to the gateway,
cates whether the task is offloaded or not. If the IoT user
which independently picks a value from the channel gain state
is selected to offload its computation task, its transmit power
space Gh obtained from cluster Ch through user clustering. gk will be selected from a discrete set Pk = { P 1 , . . . , P max}. Note is assumed to be constant during the offloading time epoch.

that the IoT user does not offload the computation task when
Pkt indicates the transmit power of the IoT user in time slot k, Ok = 0 , Ok ∈ Ok, then the cost only contains local compu-then the achievable transmission rate (bit/s) is denoted by
tation energy consumption and local task execution latency,


and the transmit power is defined as Pk =
Rk = B
1 + Pktgk
(2)
t
0 in this case.

w log2
σ 2
Ok = 1 , Ok ∈ Ok indicates that the IoT user decides to offload computation task to the gateway, with the transmit
where σ 2 indicates the power of additive white Gaussian
power Pk ∈ Pk
t
. In both cases, the computation task is executed
noise (AWGN). Then, the energy consumption of the IoT user
Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



LIU et al. : RESOURCE ALLOCATION WITH EDGE COMPUTING IN IoT NETWORKS VIA MACHINE LEARNING
3419
caused by the data transmission is indicated by Pk
Algorithm 1 Priority-Driven Clustering Based on K-Means
t , and the
transmission latency is denoted by Lk =
/
1: Input:
t
mk Rk. Similarly,
j
let fs denote the computation frequency of the edge server, Es 2: number of observed tasks J, clusters H and clustering
denotes the energy consumption per CPU cycle at the edge
feature vector, user priority xi
server. Ds indicates the computation capacity of the gateway.

3: Initialization:
The computation energy of the IoT user at the edge server is
4: Randomly initialize H cluster centroids c 1 , . . . , cH
given by Ek =
5: Repeat:
cs
fsEsmk, and the computation latency is cal-
j
culated as Lk = (
)/
6: for i = 1 : U do
s
fsmk D
j
s. Therefore, we can obtain the cost
function of the edge computing mode, and it is presented as
7:
for h = 1 : H do


8:
Calculate cluster index hi for xi as:
Ck =
+
∗
+ β
+
.

hi = arg min xi − ch2
e
Ekcs
Pkt Lkt
Lks
Lkt
(3)
2
h
9:


end for

As discussed above, the task execution is assumed to be
10: end for
successful in both cases. But there is a possibility that the
11: for h = 1 : H do
task execution is failed, like the task transmission might suf-
12:
Update the cluster centroids by
U
fer transmission outage. Then, we define a penalty function δ
c
i=1 1{ hi= h} xi
h := 	 U
as the cost when the task execution fails. Therefore, the cost
i=1 1{ hi= h}
13: end for
function is calculated in three cases as
⎧
14: Until: No change of the cluster centroids
⎨ Ck, if local
d
Ck = ⎩ Ck,
(4)
e
if edge, not failed
δ, if edge, failed .

So the computation offloading probability is defined as
⎛
⎞
∞

∞

j
j
IV. C
⎝
⎠
ENTRALIZED CLUSTERING ALGORITHMS
Pr = Pr(Ld > Lth) = Pr
Ld >
Lth
.

(5)
A. User Priority Initialization
j=1
j=1
As mentioned above, an ultradense IoT network is con-
Here, the generated tasks are independent, so the computation
sidered with a large number of IoT users denoted by U =
offloading probability can be approximately presented by the
{ u
frequency of the user’s tasks offloading. This is calculated in
1 , . . . , ui, . . . , uU }, and many IoT users are requesting for computing services from the resource-restricted edge server,
the initial process in the clustering algorithm from Fig. 2, and but it is noted that those users have different priorities of task
later it is improved and updated in each time phrase.

execution. In this section, a centralized user clustering method
is investigated at the gateway to group the IoT users according
B. Priority-Driven Clustering Based on K-Means
to user priorities which are then assumed similar across users
As discussed above, we define the user priority of compu-
in the same cluster. Here, the user priority of each IoT user
tation offloading for the IoT user ui as xi, which is indicated is defined by a tuple, including its distance di to the gate-by the feature vector, including the distance di from user
way and its computation offloading probability Pr i. We define ui to the gateway and computation offloading probability
the computation offloading probability of each IoT user as in
Pr i, that is, xi = { di, Pr i}. Therefore, a priority-driven user Definition 1.

clustering algorithm based on the standard K-means algorithm
At the IoT user ui, each task is generated with its require-
is proposed to partition IoT users into H clusters according to
ment on task execution latency, if the local computing cannot
the priorities of IoT users by minimizing the sum of squared
meet its requirement, the task has to be offloaded to the gate-
error (SSE)
way. The clustering algorithm has to reserve an initial process
H

that is used for the gateway to initialize the user priority, which
min
xi − ch2
is shown in Fig. 2. It contains that the users calculate and C
2
(6)
h,ch h=1 xi∈ C
send their computation offloading probabilities to the gate-
h
way, and the gateway records the distances from it to all the
where H is the initial number of clusters, Ch means the clus-users. After that, the gateway performs the clustering algo-
ter h, and xi is the user priority feature of user ui. Also, the rithm to group the IoT users into a bunch of clusters based
centroid of cluster ch is calculated as
on their priorities. Then, the optimal distributed computation

c
x
offloading scheme for users within the clusters (except for the
h =
1
| C
i
(7)
h|
clusters with the highest and lowest user priority) is designed
xi∈ Ch
and solved by the deep Q-network. The detailed process of the
where | · | is the cardinality and the detailed description is
designed computation offloading scheme is shown in Fig. 2.

provided in Algorithm 1.

Definition 1: Considering a typical IoT user ui as an exam-It is noted that the optimal cluster number Hop is hard to
ple, each task is generated with its task execution latency
get. One good method to validate the number of clusters is
requirement as L j
th
and the task size as mj. The task exe-
called the elbow method. Its basic idea is to run K-means clus-
cution latency at the IoT user is calculated as L j
d
= mj/Dd. tering on the data set for range values of cluster number H.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



3420
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
If the selected cluster number H is smaller than the real cluster number H, the SSEs are reduced dramatically when the cluster number increases by 1. Inversely, the SSE does not have
obvious changes. Then, the optimal cluster number is located
at the turning point, known as the elbow. The performance
index of the elbow method, i.e., SSE is defined as
H

SSE =
dist (xi, ch) 2 .

(8)
Fig. 3.

Framework of reinforcement learning for IoT networks.

h=1
However, the elbow method is sometimes ambiguous, the
V. COMPUTATION OFFLOADING SCHEME VIA DEEP
average silhouette method can be an alternative to validate
REINFORCEMENT LEARNING
the optimal cluster number jointly. This method defines a sil-
houette coefficient to measure the similarity of a user to its
After centralized user clustering, the IoT users are grouped
own cluster compared to its neighboring clusters, its silhouette
into different clusters by the gateway according to their unique
coefficient is calculated by
features, user priorities. In each cluster, each IoT user is
assumed to have the same user priorities. Here, the cluster
with the highest priority is designated as edge computing
SCi =
bi − ai , if | Ch| > 1
(9)
max{ e
while the cluster with the lowest priority is assigned as local
i, bi}
computing. For the other clusters, the optimal distributed com-
where e
putation offloading scheme solved by DRL is proposed in this
i indicates the mean distance between the user ui and
all the other users in the same cluster C
section. First, the basic formulations of reinforcement learn-
h. bi is the mean
distance between the user u
ing are shown and then a computation offloading scheme is
i and all the users in its closest
neighboring cluster C
designed with modeling the computation offloading process
l which is given by
as an MDP. Therefore, a deep Q-network-based computation

offloading algorithm is proposed to learn the optimal computa-
C
1
l = arg min
| xl − xi|2
(10)
C
tion offloading policy, which can deal with the MDP problem
h
q xl∈ Ch
with large-state space.

where q indicates the number of users in the cluster Ch.

Then, the average silhouette coefficient is obtained by cal-
A. Reinforcement Learning
culating the mean of SCi over all the users. The average
As shown in Fig. 3, reinforcement learning is the pro-silhouette coefficient is in the range [−1 , 1], the higher value
cess that the agents continuously learn optimal strategies by
means the cluster number is more appropriate for the clustering
interacting with the environment. In this scenario, each IoT
algorithm.

user is considered as an agent and everything else in the IoT
Noted that our proposed K-means algorithm is the first
network is made up of the environment. Here, due to perform-
method to cluster IoT users by defining the user priority of
ing user clustering, the scale of the computation offloading
users’ computation offloading; other explorations could be
problem in the IoT network is reduced. The computation
done in the future with different features, such as radio prop-
offloading scheme is assumed to have time slots structure
agation, usage, user mobility pattern, etc. Moreover, the user
as shown in Fig. 2. From Fig. 3, each agent, the IoT user clustering algorithm is performed periodically based on the
ui observes a state sk from state space S at time epoch k, coverage of users’ location changes and the monitored task
and then, an action ak is taken from the action space A,
types changes.

that is, decides which computing mode is taken by selecting
different transmit powers based on the policy π. Therefore,
the environment changes, new state sk+1 is observed and a
C. Complexity Analysis
reward Rke is obtained. In our scenario, the reward is desig-
Reminding that finding an optimal solution to the cluster-
nated as a negative reward, the system cost Ck, the weighted
ing problem for observations in N dimensions is an NP-hard
sum of energy consumption, and task execution latency. A
problem. If the cluster number H and the dimension N are reinforcement learning problem usually contains few key ele-fixed, the computational complexity can be exactly calculated
ments, including {Action, State, Reward, Environment}, and
as O(UNH+1 ), where U is the number of users that need to be the environment is typically formulated as an MDP.

clustered and N is the dimension of the cluster feature vector.

1) Action: At time step k, the action ak for each IoT user It depends on the two main steps: 1) calculate the cluster index
in cluster Ch is the discrete transmit power, in which Pk =
t
0
and 2) update the cluster centroids. There are many heuristic
indicates choosing local computing while others are actual
algorithms that can be used to find the optimal solution, like
transmit power in edge computing.1 In our scenario, the action Lloyd’s algorithm whose complexity is O(IUNH). Here, I is set is denoted by A = {0 , P}.

the iteration number until the algorithm is converged, which
√
has I = 2 ( (U)) iterations in the worst case.

1Assume that the transmit power is larger than 0 in edge computing.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



LIU et al. : RESOURCE ALLOCATION WITH EDGE COMPUTING IN IoT NETWORKS VIA MACHINE LEARNING
3421
2) State: The states of the agent represent the exploration
this decision-making problem since the agent cannot make
information from the environment. Here, we use the channel
predictions about what the next state and cost will be before
gain gk, task queue tk stored at the IoT user, and its remain-it takes each action.

ing computation capacity ratio rk to present the exploration
In our scenario, the IoT user is trying to design an optimal
information. Hence, the state of the user in cluster Ch at the
computation offloading scheme according to some statistical
time step k is given as
information, such as the possible channel conditions, the pos-


sible remaining computation capacity percentage of the user,
sk = gk, tk, rk
(11)
and the possible task queue, observing from the environment.

Particularly, the user is exploring the optimal policy π∗ that
where sk ∈ S, gk ∈ Gh. Gh is the channel gain set in cluster C
minimizes the long-term cost V(s, π). This means for any
h except the clusters that have the highest and lowest user
given network state s, the optimal policy π∗ can be obtained by priority, which is obtained after performing the centralized user
clustering algorithm.

π∗ = arg min V(s, π) ∀ s ∈ S.

(14)
3) Reward: The function of the reward signal is to encour-
π
age the learning algorithm to reach the goal of the optimization
The problem of designing an optimal computation task
problem. In this article, the negative reward is adopted to min-
offloading for the IoT user is a classic single-agent finite-
imize the system cost while making the right decisions on
horizon MDP problem. Then, we use the classic model-free
computation offloading and power allocation. Here, the system
reinforcement learning approach, Q-learning algorithm, to
cost is measured by the weighted sum of energy consumption
explore the optimal computation offloading policy by mini-
and task execution latency as derived in (4) in the IoT network.

mizing the long-term expected accumulated discounted cost,
To make it clear, we use reward shaping to make the reward
V(s, π). We denote Q-value, Q(s, a), as the expected accumu-more informative and accelerate the training. Therefore, the
lated discounted cost when taking an action ak ∈ A following reward for each IoT user in the cluster Ch at the time step k a policy π for a given state–action pair (s, a).

is given by
Thus, we define the action–value function Q(s, a) as
⎧


⎨ Ck, if local
d
Q(s, a) = E π Rk+1 + γ
|
.

e
Qπ sk+1 , ak+1 sk = s, ak = a
Rk =
,
e
⎩ Ck
(12)
e
if edge, 1 − p
δ,
(15)
if edge, p
where p is the failure rate of task transmission (considering the In our proposed algorithm, Q(s, a) indicates the value cal-possibility of failed task transmission) in the edge computing
culated from the cost function (4) for any given state s and mode, for simplicity, which is set as a fixed value in this article.

action a stored in the Q-table which is built up to save all the δ is the penalty function when the task execution is failed.

possible accumulative discounted cost. The Q-value is updated
Here, we denote the objective function as a negative reward,
during the time epoch if the new Q-value is smaller than the
i.e., the system cost shown in (4). In addition, the state transi-current Q-value. The Q(s, a) is updated incrementally based tion and system cost are stochastic, which can be modeled as
on the current cost function Rke and the discounted Q-value an MDP, where the state transition probabilities and the cost
Q(sk+1 , a) ∀ a ∈ A in the next time epoch. This is achieved depend only on the environment and the obtained policy. The
by the one-step Q-update equation


transition probability P = (sk+1 , Rk|
e sk, ak) is defined as the
Q sk, ak ← ( 1 − α) · Q sk, ak
transition from state sk to sk+1 with the obtained reward Rke


when the action ak is taken according to the policy. Generally,
+ α Rk + γ ·
e
min Q sk+1 , a
(16)
the MDP problem is solved by finding an optimal policy
a
that will maximize some cumulative function of the random
where Rke is the cost obtained in the current state, and α is rewards. In this case, the long-term expected cumulative neg-the learning rate ( 0 < α ≤ 1 ). Q-learning is a model-free ative reward, i.e., the system cost, over a finite horizon is
and off-policy solution to solve an MDP, in each time slot,
given by
we calculate the Q-value in the next step with all the possible


K

actions that it can take, then choose the minimum Q-value and
V(s, π) = E π
γ kRk
record the corresponding action.

e
(13)
k=1
Moreover, to explore the unknown states instead of trust-
ing the learned values of Q(s, a) completely, the -greedy
where s is the state and π is the policy, γ ∈ [0 , 1] is approach is used in the Q-learning algorithm, where the agent
the discount factor and E indicates the statistical conditional
picks a random action with small probability , or with 1 −
expectation with transition probability P.

it chooses an action that minimizes the Q(sk+1 , a) as shown Basically, the conventional solutions, like policy iteration
in (16) in each time epoch.

and value iteration [28] belonging to dynamic programming can be used to solve the MDP optimization problem with the
known state transition function. But it is hard for the agent
B. Optimality and Approximation
to know the prior information of the state transition function,
The agent in the reinforcement learning algorithm aims
which is determined by the environment. Therefore, a model-
to solve sequential decision-making problems by learning an
free reinforcement learning approach is proposed to investigate
optimal policy. In practice, the requirement for Q-learning to
Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



3422
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
obtain the correct convergence performance is that all the state
Algorithm 2 DQN Training for Computation Task Offloading
action pairs Q(s, a) continue to be updated. Moreover, if we


Initialization

explore the policy infinitely, the Q-value Q(s, a) has been val-Initialize the size of replay memory and the mini-batch,
idated to converge with possibility 1 to Q∗ (s, a), which is Initialize Q-network with random weight vector θ,
given by
Initialize parameters: discount factor γ , learning rate α,


exploration rate , and the channel gain set Gh in the
lim P

r
Q∗ (s, a) − Q(s, a)n ≥ ς = 0
(17)
clusters C
n→∞
h


Procedure

where n is the index of the obtained sample and Q∗ (s, a) is the 1: while i ≤ iteration do
optimal Q-value while Q(s, a)n is one of the obtained samples.

2:
Observe initial state sk = (gk, tk, rk)
Therefore, Q-learning can identify an optimal action-selection
3:
Select an action ak according to -greedy policy.

policy based on infinite exploration time and a partly random
4:
After taking action ak, calculate the cost Rke by (12),
policy for a finite MDP model.

and new state sk+1 = (gk+1 , tk+1 , rk+1 ) is observed.

5:
Store the state transition ( sk, ak, Rk,
e sk+1)in the replay
C. Deep Reinforcement Learning
memory
6:
Randomly sample a mini-batch of transitions from the
As mentioned above, the classical Q-learning algorithm can
experience pool
find the optimal policy when the state–action is small, and it
7:
Update weight vector θ by minimizing (18)
depends on a Q-table to store the Q-values and has to look up 8:
Every C steps update the Q-network
for every state in the table. If the state–action space becomes
9:
Update time epoch: k = k + 1
huge, it takes a long time for the Q-table to converge. The
10: end while
reason is that the states will be visited infrequently and the
corresponding Q-values are updated rarely for a large number
of states. In our problem, multiple state dimensions and con-
Algorithm 3 DQN-Based Computation Task Offloading
tinuous environment variations generate a large-state space. To
Scheme
solve this problem, a neural network can be used to approx-


Initialization

imate the Q-function, which can predict the Q values based Given an initial state s 1 = (g 1 , t 1 , r 1 ) on the input (states), that is, once the weight vector ( θ) is 1: while tk > 0 do
obtained through training, we can get the output Q values
2:
Select an action ak = min Q(sk, a, θ∗ ).

a∈ A
Q(sk, ak) quickly. Specifically, DNN has achieved more suc-
3:
State sk+1 = (gk+1 , tk+1 , rk+1 ) is observed.

cess in solving this problem, known as the deep Q-network.

4: end while
The DNN can be applied to the larger scale problems, then
it will be appropriate to address sophisticated mappings from
states to the desired Q-values output.

D. Complexity Analysis
The DQN follows the rule of neural network to update
its weight vector θ at each iteration by minimizing the loss In this section, the complexity of the proposed computa-function, which is defined as
tion task offloading algorithm is analyzed, which is mainly


determined by the training algorithm shown in Algorithm 2.

2
Loss (θ) = E
Q sk, ak − Q
For Q-learning algorithm, its time complexity depends on
target sk, ak; θ
(18)
the episodes I and the steps K in each episode, denoted as O(IK). In the deep Q-network, DNN is used to approximate
where Q target (sk, ak; θ) is the target output Q-value, and the the Q-value function in the Q-learning algorithm. Hence, its current Q-value Q (sk, ak) is the prediction which is given as time complexity mainly comes from training the DNN, which


can be calculated as O(Hi · H 1 + H 1 · H 2 + · · · + Hn · Ho).

Q sk, ak = Rk +
e
min Q sk, a, θ
(19)
a∈ A
Moreover, how many times required for training this DNN
relies on the total episodes I and the replay memory size D, where Rke is the corresponding negative reward, i.e., the system
denoted by W = (I/D). Here, the input layer of the DNN
cost.

is determined by the batch size B and state space S, i.e., Here, the agent uses a replay memory with finite size
Hi = BS, while the output layer depends on the action
to store state transitions ( sk, ak, sk+1 , ak+1 , Rke). We use the space A. Therefore, the time complexity of the proposed
experience replay technique to sample a mini-batch from
algorithm is O(W × (BS · H 1 + H 1 · H 2 + H 2 · H 3 + H 3 · A)) the memory pool, which is then used to train the DQN
in the direction of minimizing (18). The detailed process of training the DQN-based computation task offloading is
VI. NUMERICAL RESULTS
presented in Algorithm 2. After training, given an initial state, In this section, the designed computation task offload-the IoT user selects an action that has the minimum esti-
ing scheme is analyzed via the simulations. The scheme is
mated Q(sk, a, θ∗ ). Hence, the test algorithm is proposed as achieved by user clustering and DRL. Here, the user clustering
a DQN-based computation task offloading scheme shown in
is based on K-means clustering algorithms according to users’
Algorithm 3.

unique characteristics and user priorities. After performing
Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



LIU et al. : RESOURCE ALLOCATION WITH EDGE COMPUTING IN IoT NETWORKS VIA MACHINE LEARNING
3423
(a)
(b)
(c)
Fig. 4.

K-means-based user clustering and the optimal cluster number validation. (a) K-means based user clustering. (b) Optimal cluster number by SSE.

(c) Optimal cluster number by SC.

TABLE II
user clustering, the optimal distributed computation offloading
SIMULATION PARAMETERS
policy is learned with DQN for the clusters without the highest
and lowest user priority.

A. Parameters Setup
In our simulations, the distribution of the IoT users’ loca-
tions is modeled by the PCP with λ = 50. The small-scale
fading of the channels between the users and gateway is set
as the Rayleigh fading with the parameter B = 3, and the
path-loss parameter is a = 2 . 5. The input cluster number of the K-means user clustering algorithm is 4, the result of user
clustering and the validation of the optimal cluster number are
shown in Fig. 4.

For the DQN-based computation offloading algorithm, the
3) Greedy Scheme: When the task queue is not empty and
DNN with three fully connected feedforward hidden layers are
the remaining computation resource of the user is still
used to approximate the Q-function, the neurons of each hid-
enough, the IoT user decides to execute the task locally
den layer are 256, 256, and 512, respectively. The activation
or offload it to the gateway while minimizing the system
function of the first two hidden layers is rectified linear units
cost, that is, min { Ck, Ck}.

(ReLUs), and the final hidden layer employs the tanh func-
d
e
After performing user clustering, the IoT users are grouped
tion as the activation function. The replay memory is set to
into different clusters with different user priorities. A typi-
have the capacity of 1000 recent transitions due to the dramat-
cal IoT user in the cluster Ch (except the cluster with the
ically changing environment, and the mini-batch size is 300.

highest and lowest user priority) is considered as an example
The action exploration is following the -greedy policy with
to learn the optimal computation offloading policy based on
= 0 . 9, and its linearly decreasing with the iteration number. Algorithm 2.

The channel gain states between the IoT user and the gate-
First, the convergence performance of the DQN-based com-
way are set as the channel gain set obtained in the clustering
putation offloading algorithm is validated. We set the param-
algorithm in the considered cluster Ch. The other simula-
eter of task generation as τ k = 0 . 5. From Fig. 5, we can tion parameters of the proposed DQN-based computation
observe that the loss is converged to a stable value by simulat-
offloading algorithm are shown in Table II.

ing the loss function shown in (18). Based on the convergence performance of the proposed DQN-based computation offload-B. DQN-Based Computation Offloading Scheme
ing algorithm, the following simulation results are analyzed
In this section, the performance of the optimal distributed
from the trained DQN running for 2 × 104.

task offloading policy learned by the DQN-based task offload-
After the training in Algorithm 2 is finished, the compu-ing algorithm is quantified. For comparison, we also provide
tation offloading scheme is tested through Algorithm 3. We another three baselines: 1) local computing; 2) edge comput-initialize the task queue as 15 and the terminal state is set
ing; and 3) greedy scheme, which are defined as follows.

as the task queue equalling to zero for the first time. The
1) Local Computing: The computation task is locally exe-
performance of the cumulative system cost C over the expe-
cuted at the IoT user in each time slot whatever size of
rienced time slots is analyzed in Fig. 6. The comparisons of the task is generated.

the cumulative system cost among local computing, edge com-
2) Edge Computing: All the computation tasks generated at
puting, greedy scheme, and DQN-based scheme over the time
the IoT user are offloaded to the edge device and then
slots are shown in Fig. 6. The proposed DQN-based compu-processed at the edge server.

tation offloading scheme has a lower cumulative system cost
Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



3424
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
Fig. 5.

Convergence performance of the proposed DQN-based computation
Fig. 7.

Performance comparison of cumulative energy consumption Pc with
offloading algorithm measured by loss function. The weight factor β = 100.

the proposed DQN-based computation offloading scheme and the other three
baselines versus the time slots. β = 100.

Fig. 6.

Performance comparison of cumulative system cost C with the
proposed DQN-based computation offloading scheme and the other three
Fig. 8. Performance comparison of cumulative task execution latency L with baseline schemes versus the time slots. β = 100.

the proposed DQN-based computation offloading scheme and the other three
baselines versus the time slots. β = 100.

than the other three schemes. The reason is that our proposed
DQN-based scheme can learn the optimal policy to choose
higher energy consumption due to its optimal policy including
the task computing mode by minimizing the long-term system
edge computing. Here, the greedy scheme has lower energy
cost while the greedy scheme only considers the current mini-
consumption since it considers the current minimum cost.

mum system cost. Here, the higher task generation probability,
The performance of the task execution latency L among the
τk = 0 . 5, means more computation tasks are generated and three baseline schemes and our proposed DQN-based scheme executed, hence the system cost is higher.

is shown in Fig. 8. Local computing has the highest execution Fig. 7 shows the comparisons of energy consumption Ec latency since the computation ability of the IoT user is much
among edge computing, local computing, greedy scheme,
weaker than the edge server, i.e., the gateway. Specifically, the
and our proposed DQN-based scheme over the time slots.

task with a large-task size costs a large amount of time to be
We can see that the edge computing has the highest energy
executed locally. Correspondingly, the computation task can
consumption, this is because the IoT user consumes trans-
be executed more quickly when it is offloaded to the gateway
mit power to offload the computation tasks to the gateway.

and executed at the edge server. Similarly, the optimal policy
But for the local computing mode, it consumes the least
learned by our proposed DQN-based scheme includes both
energy since all the energy consumption only comes from
actions: local computing and edge computing, hence it has
the task execution. In our proposed scheme, the decisions
neutral performance of task execution latency.

on computation task offloading are made by observing envi-
Fig. 9 presents the performance comparison of average cost ronment information. Compared to local computing, it has
under different edge server computation capacity Ds. We can
Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



LIU et al. : RESOURCE ALLOCATION WITH EDGE COMPUTING IN IoT NETWORKS VIA MACHINE LEARNING
3425
to make their computation offloading decisions together, which
can be formulated as a stochastic game that makes it more
suitable for practical scenarios.

REFERENCES
[1] O. Hahm, E. Baccelli, H. Petersen, and N. Tsiftes, “Operating systems
for low-end devices in the Internet of Things: A survey,” IEEE Internet Things J. , vol. 3, no. 5, pp. 720–734, Oct. 2016.

[2] J. Dizdarević, F. Carpio, A. Jukan, and X. Masip-Bruin, “A survey of
communication protocols for Internet of Things and related challenges
of fog and cloud computing integration,” ACM Comput. Surveys, vol. 51, no. 6, p. 116, Feb. 2019.

[3] A. Yousefpour et al. , “All one needs to know about fog computing and related edge computing paradigms: A complete survey,” Sep. 2018.

[Online]. Available: arXiv:1808.05283.

[4] L. Bittencourt et al. , “The Internet of Things, fog and cloud continuum: Integration and challenges,” Internet Things, vols. 3–4, pp. 134–155, Oct. 2018.

[5] L. Liu, Z. Chang, and X. Guo, “Socially aware dynamic computation
offloading scheme for fog computing system with energy harvest-
Fig. 9.

Performance comparison of average cost under different edge server
ing devices,” IEEE Internet Things J. , vol. 5, no. 3, pp. 1869–1879, computation capacity versus time slots k. β = 50.

Jun. 2018.

[6] L. Liu, Z. Chang, X. Guo, S. Mao, and T. Ristaniemi, “Multiobjective
optimization for computation offloading in fog computing,” IEEE
observe that the average cost is smaller when the edge server
Internet Things J. , vol. 5, no. 1, pp. 283–294, Feb. 2018.

[7] C.-H. Hong and B. Varghese, “Resource management in fog/edge com-
has a larger computation capacity Ds = 20 GHz compared
puting: A survey,” Sep. 2018. [Online]. Available: arXiv:1810.00305.

to Ds = 4 GHz. This is because more powerful computation
[8] P. Mach and Z. Becvar, “Mobile edge computing: A survey on archi-
capacity can provide faster task execution, that is, smaller task
tecture and computation offloading,” Mar. 2017. [Online]. Available:
arXiv:1702.05309.

execution latency.

[9] Y. Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief, “Mobile edge
computing: Survey and research outlook,” Jan. 2017. [Online]. Available:
arXiv:1701.01090.

VII. CONCLUSION
[10] S. Bugdary and S. Maymon, “Online clustering by penalized weighted
GMM,” Feb. 2019. [Online]. Available: arXiv:1902.02544.

In this article, we designed the optimal computation
[11] S. Bi and Y. J. Zhang, “Computation rate maximization for wireless pow-task offloading schemes for ultradense IoT edge comput-
ered mobile-edge computing with binary computation offloading,” IEEE
ing networks by considering the statistics of environment
Trans. Wireless Commun. , vol. 17, no. 6, pp. 4177–4190, Jun. 2018.

[12] X. He, H. Xing, Y. Chen, and A. Nallanathan, “Energy-efficient
information, including the time-varying channel conditions,
mobile-edge computation offloading for applications with shared data,”
the dynamic task queue and the remaining computation capac-
Sep. 2018. [Online]. Available: arXiv:1809.00966.

ity of the IoT users via machine learning approaches. The
[13] J. Xu and S. Ren, “Online learning for offloading and autoscaling in
computation offloading scheme was developed by two steps:
renewable-powered mobile edge computing,” in Proc. Global Commun.

Conf. (GLOBECOM), Washington, DC, USA, Feb. 2016, pp. 1–6.

1) centralized user clustering and 2) distributed computa-
[14] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Performance
tion offloading scheme. The centralized user clustering was
optimization in mobile-edge computing via deep reinforcement learn-
achieved by using the K-means clustering algorithm with user
ing,” Mar. 2018. [Online]. Available: arXiv:1804.00514.

[15] Y. Mao, J. Zhang, and K. B. Letaief, “Joint task offloading scheduling priorities as the clustering feature. With user clustering, the
and transmit power allocation for mobile-edge computing systems,” in
clusters with the highest and lowest user priority were assigned
Proc. Wireless Commun. Netw. Conf. (WCNC), San Francisco, CA, USA, as edge computing and local computing, respectively. For a
Jan. 2017, pp. 1–6.

[16] S. Sardellitti, G. Scutari, and S. Barbarossa, “Joint optimization of radio typical user in any other clusters, an optimal distributed com-and computational resources for multicell mobile-edge computing,”
putation offloading policy was learned by using DQN with
IEEE Trans. Signal Inf. Process. Over Netw. , vol. 1, no. 2, pp. 89–103, minimizing the long-term system cost. The K-means user clus-Jun. 2015.

[17] A. Al-Shuwaili and O. Simeone, “Energy-efficient resource allocation
tering algorithm has been demonstrated by the simulations
for mobile edge computing-based augmented reality applications,” IEEE
and the optimal cluster number was validated. Moreover, the
Wireless Commun. Lett. , vol. 6, no. 3, pp. 398–401, Jun. 2017.

DQN-based computation offloading algorithm has been sim-
[18] Z. Liang, Y. Liu, T.-M. Lok, and K. Huang, “Multiuser computation
offloading and downloading for edge computing with virtualization,”
ulated, it has lower system cost compared to the other three
Nov. 2018. [Online]. Available: arXiv:1811.07517.

baseline schemes and a neutral performance was obtained with
[19] C. You and K. Huang, “Multiuser resource allocation for mobile-
separately considering energy consumption and task execution
edge computation offloading,” in Proc. Global Commun. Conf.

(GLOBECOM), Washington, DC, USA, Dec. 2016, pp. 1–6.

latency.

[20] D. Borthakur, H. Dubey, N. Constant, L. Mahler, and K. Mankodiya,
In this article, we deal with the massive computation
“Smart fog: Fog computing framework for unsupervised clustering ana-
offloading in IoT networks from the aspect of user cluster-
lytics in wearable Internet of Things,” in Proc. Global Conf. Signal Inf.

Process. (GlobalSIP), Montreal, QC, Canada, Nov. 2017, pp. 472–476.

ing, and then a distributed computation offloading scheme
[21] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, “Towards
is designed for users required special computation offloading
an intelligent edge: Wireless communication meets machine learning,”
decisions. We consider a specific user makes decisions on its
Sep. 2018. [Online]. Available: arXiv:1809.00343.

[22] Y. Du and K. Huang, “Fast analog transmission for high-mobility wire-
computation task execution by interacting with the dynamic
less data acquisition in edge learning,” Jul. 2018. [Online]. Available:
environment. In our next work, we will consider multiple users
arXiv:1807.11250.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



3426
IEEE INTERNET OF THINGS JOURNAL, VOL. 7, NO. 4, APRIL 2020
[23] C. Zhang, Z. Liu, B. Gu, K. Yamori, and Y. Tanaka, “A deep rein-
Jian Wang (Member, IEEE) received the Ph.D.

forcement learning based approach for cost-and energy-aware multi-flow
degree from Nanjing University, Nanjing, China, in
mobile data offloading,” IEICE Trans. Commun. , vol. E101.B, no. 7, 2006.

pp. 1625–1634, Jan. 2018.

He is currently an Associate Professor with
[24] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen,
the School of Electronic Science and Engineering
“In-edge AI: Intelligentizing mobile edge computing, caching and com-
and the Institute of Space-Terrestrial Intelligent
munication by federated learning,” Sep. 2018. [Online]. Available:
Networks, Nanjing University. His research interests
arXiv:1809.07857.

include mobile ad hoc networks, spatial information
[25] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized
network, satellite communications and networks,
computation offloading performance in virtual edge computing systems
video coding, and transmission.

via deep reinforcement learning,” IEEE Internet Things J. , vol. 6, no. 3, pp. 4005–4018, Jun. 2019.

[26] J. Chen, S. Chen, Q. Wang, B. Cao, G. Feng, and J. Hu, “iRAF: A deep
reinforcement learning approach for collaborative mobile edge comput-
ing IoT networks,” IEEE Internet Things J. , vol. 6, no. 4, pp. 7011–7024, Apr. 2019.

[27] M. Min, L. Xiao, Y. Chen, P. Cheng, D. Wu, and W. Zhuang, “Learning-
based computation offloading for IoT devices with energy harvesting,”
IEEE Trans. Veh. Technol. , vol. 68, no. 2, pp. 1930–1941, Feb. 2019.

[28] M. L. Puterman, Markov Decision Processes: Discrete Stochastic
Dynamic Programming. New York, NY, USA: Wiley, 2014.

Xiaolan Liu (Student Member, IEEE) received the
Yue Gao (Senior Member, IEEE) received the Ph.D.

B.S. and M.S. degrees in communication engi-
degree from the Queen Mary University of London
neering from Jilin University, Changchun, China.

(QMUL), London, U.K., in 2007.

She is currently pursuing the Ph.D. degree with
He was a Lecturer, a Senior Lecturer, and a Reader
the Department of Electronic Engineering and
of antennas and signal processing with QMUL. He is
Computer Science, Queen Mary University of
currently a Professor and a Chair of wireless com-
London, London, U.K.

munications with the Institute for Communication
Her research interests include energy harvesting
Systems, University of Surrey, Guildford, U.K. He
and machine learning for IoT networks with low-
has published over 180 peer-reviewed journal and
power communication technology, and millimeter
conference papers, two patents, one book, and five
wave communications.

book chapters. He focuses on developing fundamen-
tal research into practice in the interdisciplinary area of smart antennas, signal processing, spectrum sharing, millimeter wave, and Internet-of-Things technologies in mobile and satellite systems.

Prof. Gao was a co-recipient of the EU Horizon Prize Award on
Jiadong Yu (Student Member, IEEE) received the
Collaborative Spectrum Sharing in 2016. He served as the Signal Processing B.S. degree in communication engineering from
for Communications Symposium Co-Chair for IEEE ICCC 2016, the
Dalian Maritime University, Dalian, China, and the
Publicity Co-Chair for IEEE GLOBECOM 2016, the Cognitive Radio
M.S. degree (with Distinction) in wireless com-
Symposium Co-Chair for IEEE GLOBECOM 2017, and the General Chair
munication from the University of Southampton,
for IEEE WoWMoM and iWEM 2017. He is the Chair of the IEEE
Southampton, U.K. She is currently pursuing the
Technical Committee on Cognitive Networks and the IEEE Distinguished
Ph.D. degree with the Department of Electronic
Lecturer of the Vehicular Technology Society. He is an Editor of the
Engineering and Computer Science, Queen Mary
IEEE INTERNET OF THINGS JOURNAL, the IEEE TRANSACTIONS ON
University of London, London, U.K.

VEHICULAR TECHNOLOGY, and the IEEE TRANSACTIONS ON COGNITIVE
Her
research
interests
include
compressive
NETWORKS. He is an Engineering and Physical Sciences Research Council
sensing and millimeter-wave communications.

Fellow from 2018 to 2023.

Authorized licensed use limited to: National Taipei Univ. of Technology. Downloaded on May 28,2023 at 08:36:49 UTC from IEEE Xplore. Restrictions apply.



